{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Test S3- Glue- RDSinstance- IAM-securityGrups- Postgres-Lambda- API RESTful\n##### flujo de trabajo:\n- Almacenamiento de Datos: Uso **pgvector en Postgres** para almacenar datos médicos (con embeddings vectoriales).\n- ETL con AWS Glue: AWS glue para leer datos de s3, transformarlos y cargarlos en postgress.\n- Consultas de similaridad: uso consultas SQL con **pgvector** para encontrar enfermedades similares basadas en sus embeddings.\n- Lambda para API: Implementar una API RESTful en AWS Lambda para hacer consultas en tiempo real.",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nimport boto3\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1bc1bf08-ef3a-4246-b6c8-6f12348af977.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1bc1bf08-ef3a-4246-b6c8-6f12348af977.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 5.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1bc1bf08-ef3a-4246-b6c8-6f12348af977.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 1bc1bf08-ef3a-4246-b6c8-6f12348af977.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Obtener los argumentos necesarios\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "GlueArgumentError: the following arguments are required: --JOB_NAME\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Crear o reutilizar el SparkContext y GlueContext\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Crear el objeto 'job' e inicializarlo\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "NameError: name 'args' is not defined\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": " ### Ingesta y transformación de datos",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Leer el CSV desde S3\ndf = spark.read.option(\"header\", \"true\").csv(\"s3://source-embeddings-ohio/medical_data.csv\")\n\n# Transformar la columna de embeddings (convertir el texto a un array de float)\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, FloatType\n\ndef parse_embedding(embedding_str):\n    return list(map(float, embedding_str.strip(\"[]\").split(\", \")))\n\nparse_embedding_udf = udf(parse_embedding, ArrayType(FloatType()))\ndf = df.withColumn(\"embedding\", parse_embedding_udf(df[\"embedding\"]))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Conexion en tabla en PostgreSQL",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Conexión a la base de datos Aurora PostgreSQL\nconnection_options = {\n    \"url\": \"jdbc:postgresql://db-identifier-aws-postgre.cj6s66gsqow2.us-east-2.rds.amazonaws.com:5432/sampleDB\",\n    \"user\": \"postgres\",\n    \"password\": \"Adri2319$\",\n    \"dbtable\": \"test_embeddings\", \n    \"driver\": \"org.postgresql.Driver\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 34,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### carga",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Cargar datos en la base de datos Aurora\ndf.write.format(\"jdbc\").options(**connection_options).mode(\"append\").save()\n\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "**DONE!**",
			"metadata": {}
		}
	]
}